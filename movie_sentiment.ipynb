{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7672e5e7",
   "metadata": {},
   "source": [
    "# Movie Sentiment Analysis\n",
    "\n",
    "Goal: Use pre-trained models distillBERT and fine tune it with IMBD movie reviews so that it can give sentiment analysis of reviews.\n",
    "\n",
    "Outcome: The sentiment analysis should give a positive or negative sentiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eaf5d7",
   "metadata": {},
   "source": [
    "# Summary of Work\n",
    "\n",
    "In this task, I fine-tuned a pre-trained transformer model to perform sentiment analysis on movie reviews. While BERT was initially considered, its large size made it less practical for this project, so I opted to use DistilBERT, a smaller and more efficient distilled version of BERT.\n",
    "\n",
    "After selecting the model, I sourced the IMDB movie review dataset from Kaggle. The dataset was organized into two columns, review and sentiment, with sentiment labels of positive or negative. Although the data was already labeled, some preprocessing was required to prepare it for training with DistilBERT.\n",
    "\n",
    "First, the dataset was split into training and testing sets using train_test_split, with 20% of the data reserved for evaluation. The column names were then updated to text and label to match the format expected by the model. Once properly formatted, the data was tokenized using the tokenizer from the pre-trained DistilBERT model.\n",
    "\n",
    "With the data prepared, training arguments were defined and the model was fine-tuned using the Hugging Face Trainer class. This approach simplified the training process by handling the training loop and evaluation internally, allowing the focus to remain on model configuration rather than low-level implementation.\n",
    "\n",
    "In the end, the model was trained on the IMDB dataset and achieved an accuracy of approximately 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4957a8e",
   "metadata": {},
   "source": [
    "# What I Learned\n",
    "\n",
    "This task may seem simple, and at first glance it might not look like much was done, but I ended up spending a large portion of my time trying to understand how the model worked and digging into the finer details of the code. Along the way, I ran into several issues that took time to identify and fully understand.\n",
    "\n",
    "To begin with, the output of the sentiment analysis was not using the correct labels. Instead of returning positive or negative, the model was outputting LABEL_0 and LABEL_1. I learned how to modify the modelâ€™s configuration so that it would return human-readable labels instead. This was done by adding 'id2label' argument to the model config file that mapped that 0 and 1 to 'negative' and 'positve', respectively.\n",
    "\n",
    "I also learned that the Trainer class from Hugging Face requires the dataset to include a labels column, which meant I had to map the original sentiment values to numerical labels and update the dataset accordingly. In addition, the IMDB movie review dataset was very large, which caused issues during tokenization. The tokenizer has a maximum sequence length of 512 tokens, and many of the reviews exceeded this limit, so I learned how and why truncation is required for the model to work properly and that you can even reduce the length to much smaller size to try to speed up training.\n",
    "\n",
    "Beyond data handling, I also explored transfer learning concepts, including how to freeze certain layers of the model to reduce training time. Finally, I gained experience working with the TrainingArguments class and learned how different parameters affect training and evaluation behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d40b0",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "As for future work, the next step would be to take the sentiment analysis further by applying it to live internet reviews. This would involve adding functionality to extract movie reviews from platforms like Reddit or Twitter (now known as X) and using that data to further retrain or fine-tune the model.\n",
    "\n",
    "It would also be interesting to perform large-scale sentiment analysis across many reviews and generate a concise overall sentiment for a movie based on the proportion of positive and negative feedback found online. In addition, presenting the results as a numeric score could make the output more intuitive and easier for users to interpret.\n",
    "\n",
    "As a side note, I also think it be fun to give the sentiment anlaysis output in meme form. I think it be a funny way to give overall sentiment on a particular movie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa8c20",
   "metadata": {},
   "source": [
    "## Data processing and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9decd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "from datasets import load_dataset\n",
    "\n",
    "imbd_raw_data = load_dataset('csv', data_files='IMBD_Dataset.csv')\n",
    "imbd_dataset = imbd_raw_data['train'].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef72a4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d95230de3a54cbbbbe3d48ed6eca1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9366cb9f0bbb467d9237f22f65b92432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_map = {\n",
    "    \"negative\": 0,\n",
    "    \"positive\": 1,\n",
    "}\n",
    "\n",
    "def encode_labels(example):\n",
    "    example[\"text\"] = example[\"review\"]\n",
    "    example[\"labels\"] = label_map[example[\"sentiment\"]]\n",
    "    return example\n",
    "\n",
    "imbd_train_test_data = imbd_dataset.map(encode_labels).remove_columns(['review','sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c690e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DistilBertConfig\n",
    "\n",
    "model_name = \"distilbert/distilbert-base-uncased\"\n",
    "\n",
    "my_config = DistilBertConfig.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2,\n",
    "    id2label={\n",
    "        0: \"negative\",\n",
    "        1: \"positive\",\n",
    "    },\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config=my_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6d23347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ade3ef100948f19251149ba4bf7da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d9d1dde24a478399643c740e4ce56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_data(dataset):\n",
    "    return tokenizer(dataset[\"text\"], truncation=True, padding=True, max_length=256)\n",
    "\n",
    "imbd_data = imbd_train_test_data.map(tokenize_data, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b331184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95611a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return { \n",
    "        \"accuracy\" : accuracy_score(labels, preds),\n",
    "        \"precision\" : precision_score(labels, preds),\n",
    "        \"f1_score\" : f1_score(labels, preds),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0db3fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze base model parameters\n",
    "for name, param in model.base_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# keep classifier trainable\n",
    "for name, param in model.base_model.named_parameters():\n",
    "    if \"transformer.layer.5\" in name or \"classifier\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43fa318",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acd724a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-imbd-dataset\",\n",
    "    save_strategy=\"best\",\n",
    "    eval_strategy=\"steps\",\n",
    "    metric_for_best_model=\"loss\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "403f308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=imbd_data[\"train\"],\n",
    "    eval_dataset=imbd_data[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35b258f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 1:48:51, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.274759</td>\n",
       "      <td>0.884200</td>\n",
       "      <td>0.894437</td>\n",
       "      <td>0.883454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.282800</td>\n",
       "      <td>0.263190</td>\n",
       "      <td>0.891500</td>\n",
       "      <td>0.877489</td>\n",
       "      <td>0.894177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.277100</td>\n",
       "      <td>0.255893</td>\n",
       "      <td>0.896200</td>\n",
       "      <td>0.886950</td>\n",
       "      <td>0.898095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.250657</td>\n",
       "      <td>0.896900</td>\n",
       "      <td>0.891961</td>\n",
       "      <td>0.898213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>0.249731</td>\n",
       "      <td>0.897500</td>\n",
       "      <td>0.891628</td>\n",
       "      <td>0.898925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.2934693115234375, metrics={'train_runtime': 6532.8263, 'train_samples_per_second': 12.246, 'train_steps_per_second': 0.383, 'total_flos': 5298695946240000.0, 'train_loss': 0.2934693115234375, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9ba7d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 01:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2497306913137436,\n",
       " 'eval_accuracy': 0.8975,\n",
       " 'eval_precision': 0.8916275430359938,\n",
       " 'eval_f1_score': 0.8989251553101272,\n",
       " 'eval_runtime': 83.2547,\n",
       " 'eval_samples_per_second': 120.113,\n",
       " 'eval_steps_per_second': 3.76,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e469ca",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2e0ea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9619709849357605}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-imbd-dataset/checkpoint-2500\"\n",
    ")\n",
    "\n",
    "classifier(\"This movie was surprisingly bad.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "202ce64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9867086410522461}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-imbd-dataset/checkpoint-2500\"\n",
    ")\n",
    "\n",
    "classifier(\"This movie was the wonderful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a86ba",
   "metadata": {},
   "source": [
    "# Store Model and Dataset to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5482588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7193c78fb39045f59c2e3123e47e6d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc1435dceeb4061b1299fe5a9f80ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/acosio14/movie_sentiment-distilbert/commit/6f4dc3a4b94aa3c725b1f18d177f66670953cf80', commit_message='Upload tokenizer', commit_description='', oid='6f4dc3a4b94aa3c725b1f18d177f66670953cf80', pr_url=None, repo_url=RepoUrl('https://huggingface.co/acosio14/movie_sentiment-distilbert', endpoint='https://huggingface.co', repo_type='model', repo_id='acosio14/movie_sentiment-distilbert'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"movie-sentiment-distilbert\")\n",
    "tokenizer.push_to_hub(\"movie_sentiment-distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9abce798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701bed0f3c3846dfbeb764836ec221fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282ec93829134c4ea7d2aed1a3978f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de00d3a150344c0bd0f025868b14bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee22d84bcfd4582b213a3b27f1abe79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a892f395cf646d0a2e5d7330ba6e51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224c1fea9f1548a2a0093b6a1b0ca17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bca695b758047f9b43e3edeef5a3b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f158a6b2eb0b4e93a4a3b0544c5c945a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/acosio14/imbd-movie-reviews/commit/5dfd25cf1162851cd6fcea5e1950c277382aa69f', commit_message='Upload dataset', commit_description='', oid='5dfd25cf1162851cd6fcea5e1950c277382aa69f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/acosio14/imbd-movie-reviews', endpoint='https://huggingface.co', repo_type='dataset', repo_id='acosio14/imbd-movie-reviews'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imbd_data.push_to_hub(\"acosio14/imbd-movie-reviews\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie-sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
